{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn==1.6.0\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51452b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='/Users/jathin/Desktop/sentiment analysis/archive/TRAIN.csv'\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56a4b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print((df['Class']== 'Positive').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)\n",
    "print(\"Number of unique classes:\", len(class_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34725d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"TRAIN.csv\")  # Replace with your actual file name\n",
    "print(train_df.head())  # Show first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55458c3c",
   "metadata": {},
   "source": [
    "To convert audio to Spectrogram use libraries such as librosa, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert a single audio file to a spectrogram image\n",
    "def audio_to_spectrogram(audio_path, save_path, img_size=(128, 128)):\n",
    "    y, sr = librosa.load(audio_path, sr=None)  # Load the audio file\n",
    "    # Generate mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)  # Convert to dB scale\n",
    "    \n",
    "    # Create and save the plot (no axes for clean image)\n",
    "    plt.figure(figsize=(img_size[0]/100, img_size[1]/100), dpi=100)\n",
    "    plt.axis('off')\n",
    "    librosa.display.specshow(S_dB, sr=sr, cmap='viridis')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Function to convert all audio files in a folder to spectrograms\n",
    "def convert_folder_to_spectrograms(audio_folder, output_folder, image_extension='.png', img_size=(128, 128)):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    audio_files = [f for f in os.listdir(audio_folder) if f.lower().endswith('.wav')]\n",
    "    if not audio_files:\n",
    "        print(\"No WAV files found in the folder.\")\n",
    "        return\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        audio_path = os.path.join(audio_folder, audio_file)\n",
    "        # Create output filename (e.g., 'audio.wav' -> 'audio.png')\n",
    "        save_filename = os.path.splitext(audio_file)[0] + image_extension\n",
    "        save_path = os.path.join(output_folder, save_filename)\n",
    "        \n",
    "        audio_to_spectrogram(audio_path, save_path, img_size)\n",
    "        print(f\"Converted {audio_file} to spectrogram: {save_filename}\")\n",
    "\n",
    "# Example usage - replace with your actual folders\n",
    "# convert_folder_to_spectrograms('path/to/your/audio_folder', 'path/to/output_spectrograms_folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89810ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/yourenvname/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 390ms/step - accuracy: 0.3624 - loss: 1.2207\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 318ms/step - accuracy: 0.7170 - loss: 0.7907\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 278ms/step - accuracy: 0.8069 - loss: 0.5116\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 271ms/step - accuracy: 0.8345 - loss: 0.4642\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 289ms/step - accuracy: 0.9248 - loss: 0.2627\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 273ms/step - accuracy: 0.9417 - loss: 0.1599\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 292ms/step - accuracy: 0.9790 - loss: 0.0868\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 294ms/step - accuracy: 0.9905 - loss: 0.0487\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 302ms/step - accuracy: 0.9881 - loss: 0.0429\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 274ms/step - accuracy: 0.9916 - loss: 0.0334\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 285ms/step - accuracy: 0.9953 - loss: 0.0353\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 283ms/step - accuracy: 0.9986 - loss: 0.0212\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 396ms/step - accuracy: 1.0000 - loss: 0.0103\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 326ms/step - accuracy: 1.0000 - loss: 0.0081\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 334ms/step - accuracy: 1.0000 - loss: 0.0076\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 348ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 427ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 500ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 339ms/step - accuracy: 1.0000 - loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved as 'sentiment_model.h5'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths (adjust if needed)\n",
    "csv_path = 'TRAIN.csv'  # Your training CSV with Filename and Class\n",
    "images_folder = 'train_images'  # Folder with training spectrogram images\n",
    "image_extension = '.png'  # Change to '.jpg' if necessary\n",
    "img_size = (128, 128)  # Resize images to this size\n",
    "\n",
    "# Step 1: Load CSV and prepare labels\n",
    "df = pd.read_csv(csv_path)\n",
    "labels = df['Class'].values\n",
    "filenames = df['Filename'].values\n",
    "\n",
    "# Map filenames to image paths (assuming images like '346.png' for '346.wav')\n",
    "image_paths = [os.path.join(images_folder, f.replace('.wav', image_extension)) for f in filenames]\n",
    "\n",
    "# Load images\n",
    "images = []\n",
    "valid_labels = []\n",
    "for i, path in enumerate(image_paths):\n",
    "    if os.path.exists(path):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "        valid_labels.append(labels[i])\n",
    "    else:\n",
    "        print(f\"Warning: Image not found: {path}\")\n",
    "\n",
    "images = np.array(images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "# Encode labels (Positive=0, Negative=1, Neutral=2)\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(valid_labels)\n",
    "encoded_labels = to_categorical(encoded_labels, num_classes=3)\n",
    "\n",
    "# Use full data for training (no split, as per your request)\n",
    "X_train = images\n",
    "y_train = encoded_labels\n",
    "\n",
    "# Step 2: Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
    "\n",
    "# Step 4: Save the model\n",
    "model.save('sentiment_model.h5')\n",
    "print(\"Model training complete and saved as 'sentiment_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dd382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Distinct testing variables\n",
    "test_images_folder = 'test_images'\n",
    "test_image_extension = '.png'  # Change if needed\n",
    "test_img_size = (128, 128)\n",
    "test_model_path = 'sentiment_model.h5'\n",
    "test_output_csv = 'test_predictions.csv'\n",
    "\n",
    "# Load model\n",
    "test_model = load_model(test_model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Load test images directly from folder\n",
    "test_filenames = [f for f in os.listdir(test_images_folder) if f.endswith(test_image_extension)]\n",
    "test_image_paths = [os.path.join(test_images_folder, f) for f in test_filenames]\n",
    "\n",
    "test_images = []\n",
    "test_valid_filenames = []\n",
    "for path in test_image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img, test_img_size)\n",
    "        img = img / 255.0\n",
    "        test_images.append(img)\n",
    "        test_valid_filenames.append(os.path.basename(path))\n",
    "    else:\n",
    "        print(f\"Warning: Could not read image: {path}\")\n",
    "\n",
    "if not test_images:\n",
    "    print(\"No valid images found. Check folder and extension.\")\n",
    "    exit()\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = test_model.predict(test_images)\n",
    "test_predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Decode to string labels (match training classes)\n",
    "test_le = LabelEncoder()\n",
    "test_le.fit(['Positive', 'Negative', 'Neutral'])  # Assumes these classes from training\n",
    "test_predicted_labels = test_le.inverse_transform(test_predicted_classes)\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Filename': test_valid_filenames,\n",
    "    'PredictedClass': test_predicted_labels\n",
    "})\n",
    "test_results_df.to_csv(test_output_csv, index=False)\n",
    "print(f\"Predictions saved to '{test_output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee42694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Test Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00        36\n",
      "     Neutral       1.00      1.00      1.00        39\n",
      "    Positive       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00       110\n",
      "   macro avg       1.00      1.00      1.00       110\n",
      "weighted avg       1.00      1.00      1.00       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36  0  0]\n",
      " [ 0 39  0]\n",
      " [ 0  0 35]]\n",
      "\n",
      "Predictions and actuals saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Paths (adjust if needed)\n",
    "test_csv_path = '/Users/jathin/Desktop/sentiment analysis/archive/test_predictions.csv'  # Your test CSV with Filename and Class\n",
    "test_images_folder = 'test_images'  # Folder with test spectrogram images\n",
    "test_image_extension = '.png'  # Change to '.jpg' if necessary\n",
    "test_img_size = (128, 128)  # Must match training size\n",
    "test_model_path = 'sentiment_model.h5'  # Trained model\n",
    "output_predictions_csv = 'test_predictions.csv'  # Optional output\n",
    "\n",
    "# Step 1: Load the trained model\n",
    "model = load_model(test_model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Step 2: Load test CSV for actual labels\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "# Standardize labels (e.g., handle case/whitespace)\n",
    "test_df['PredictedClass'] = test_df['PredictedClass'].str.strip().str.title()\n",
    "\n",
    "# Step 3: Load and preprocess test images, aligning with CSV\n",
    "test_filenames_csv = test_df['Filename'].values  # From CSV (e.g., '346.wav')\n",
    "test_image_paths = [os.path.join(test_images_folder, f.replace('.wav', test_image_extension)) for f in test_filenames_csv]\n",
    "\n",
    "test_images = []\n",
    "test_valid_filenames = []\n",
    "test_actual_labels = []\n",
    "for i, path in enumerate(test_image_paths):\n",
    "    if os.path.exists(path):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, test_img_size)\n",
    "        img = img / 255.0  # Normalize\n",
    "        test_images.append(img)\n",
    "        test_valid_filenames.append(test_filenames_csv[i])\n",
    "        test_actual_labels.append(test_df['PredictedClass'].iloc[i])\n",
    "    else:\n",
    "        print(f\"Warning: Image not found: {path}\")\n",
    "\n",
    "if not test_images:\n",
    "    print(\"No valid images found. Check paths and filenames.\")\n",
    "    exit()\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_actual_labels = np.array(test_actual_labels)\n",
    "\n",
    "# Encode actual labels (match training order)\n",
    "le = LabelEncoder()\n",
    "le.fit(['Positive', 'Negative', 'Neutral'])\n",
    "encoded_actual_labels = le.transform(test_actual_labels)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_labels = le.inverse_transform(predicted_classes)\n",
    "\n",
    "# Step 5: Compare and compute metrics\n",
    "accuracy = accuracy_score(encoded_actual_labels, predicted_classes)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(encoded_actual_labels, predicted_classes, target_names=le.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(encoded_actual_labels, predicted_classes))\n",
    "\n",
    "# Optional: Save predictions to CSV for reference\n",
    "results_df = pd.DataFrame({\n",
    "    'Filename': test_valid_filenames,\n",
    "    'ActualClass': test_actual_labels,\n",
    "    'PredictedClass': predicted_labels\n",
    "})\n",
    "results_df.to_csv(output_predictions_csv, index=False)\n",
    "print(f\"\\nPredictions and actuals saved to '{output_predictions_csv}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourenvname",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
